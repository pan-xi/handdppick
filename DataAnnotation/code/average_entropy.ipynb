{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m948.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=ea6654d2d4e54036533bcad551493f3a272fb04274cdd10ff2ec901727444420\n",
      "  Stored in directory: /home/casit205/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import jieba\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/casit205/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/casit205/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file_path):\n",
    "    with open(file_path,'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    function_description = []\n",
    "    vulnerability_analysis = []\n",
    "    repaired_code = []\n",
    "    for item in data:\n",
    "        function_description.append(item['function_description'])\n",
    "        vulnerability_analysis.append(item['vulnerability_analysis'])\n",
    "        repaired_code.append(item['repaired_code'])\n",
    "    return function_description, vulnerability_analysis, repaired_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(char):\n",
    "    \"\"\"判断一个字符是否是中文\"\"\"\n",
    "    return '\\u4e00' <= char <= '\\u9fa5'\n",
    "\n",
    "def mixed_tokenize(text):\n",
    "    \"\"\"混合分词，英文用 nltk 分词，中文用 jieba 分词\"\"\"\n",
    "    tokens = []\n",
    "    # 使用正则表达式分割中文和非中文\n",
    "    parts = re.split(r'([\\u4e00-\\u9fa5]+)', text)\n",
    "    for part in parts:\n",
    "        if part:  # 如果 part 非空\n",
    "            if re.search(r'[\\u4e00-\\u9fa5]', part):  # 如果包含中文\n",
    "                tokens.extend(jieba.lcut(part))\n",
    "            else:  # 否则，当英文处理\n",
    "                tokens.extend(nltk.word_tokenize(part))\n",
    "    return tokens\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"计算一个字符串的熵（混合分词）\"\"\"\n",
    "    tokens = mixed_tokenize(text)\n",
    "    token_counts = Counter(tokens)\n",
    "    total_tokens = len(tokens)\n",
    "    entropy = 0\n",
    "    for count in token_counts.values():\n",
    "        probability = count / total_tokens\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def calculate_average_entropy(responses):\n",
    "    \"\"\"计算多个回复的平均熵\"\"\"\n",
    "    entropies = [calculate_entropy(str(response)) for response in responses]\n",
    "    return np.mean(entropies), np.std(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_description: 平均熵 = 3.4890293518505495 , 标准差 = 0.38095940555415836\n",
      "vulnerability_analysis: 平均熵 = 5.003193624166888 , 标准差 = 1.0044439479472367\n",
      "repaired_code: 平均熵 = 5.506753289207661 , 标准差 = 1.0981017160249487\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "        熵值越高，说明信息量越大，信息越不确定，信息的不确定性越大，重复性越低，词汇多样性越高。\n",
    "        熵值标准差越大，说明回复的差异性越大。一致性越低。\n",
    "    \"\"\"\n",
    "    function_description, vulnerability_analysis, repaired_code = get_words('./small_sample_output_dir/split0_output_deepseek-coder.json')\n",
    "\n",
    "    avg_entropy_fd, std_entropy_fd = calculate_average_entropy(function_description)\n",
    "    avg_entropy_va, std_entropy_va = calculate_average_entropy(vulnerability_analysis)\n",
    "    avg_entropy_rc, std_entropy_rc = calculate_average_entropy(repaired_code)\n",
    "    \n",
    "    print('function_description: 平均熵 =', avg_entropy_fd, ', 标准差 =', std_entropy_fd)\n",
    "    print('vulnerability_analysis: 平均熵 =', avg_entropy_va, ', 标准差 =', std_entropy_va)\n",
    "    print('repaired_code: 平均熵 =', avg_entropy_rc, ', 标准差 =', std_entropy_rc) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
